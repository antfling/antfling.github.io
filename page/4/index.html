<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"antfling.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Es Ist Vorbei">
<meta name="keywords" content="Blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Antfling">
<meta property="og:url" content="https://antfling.github.io/page/4/index.html">
<meta property="og:site_name" content="Antfling">
<meta property="og:description" content="Es Ist Vorbei">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Antfling">
<meta name="twitter:description" content="Es Ist Vorbei">

<link rel="canonical" href="https://antfling.github.io/page/4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Antfling</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Antfling</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">雄关漫道真如铁</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://antfling.github.io/2016/12/31/HbaseInstall/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ant">
      <meta itemprop="description" content="Es Ist Vorbei">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Antfling">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/12/31/HbaseInstall/" class="post-title-link" itemprop="url">Hbase分布式安装</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2016-12-31 18:35:00" itemprop="dateCreated datePublished" datetime="2016-12-31T18:35:00+08:00">2016-12-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-09-29 14:52:07" itemprop="dateModified" datetime="2020-09-29T14:52:07+08:00">2020-09-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/BigData/" itemprop="url" rel="index"><span itemprop="name">BigData</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Hbase分布式安装"><a href="#Hbase分布式安装" class="headerlink" title="Hbase分布式安装"></a>Hbase分布式安装</h2><h3 id="1-准备"><a href="#1-准备" class="headerlink" title="1.准备"></a>1.准备</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">请确保已经安装过hadoop和jdk</span><br></pre></td></tr></table></figure>

<h3 id="2-基本安装"><a href="#2-基本安装" class="headerlink" title="2.基本安装"></a>2.基本安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ tar zxvf hbase-version.tar.gz #解压</span><br><span class="line">$ sudo mv hbase-version /opt/hbase #移动到/opt文件夹下</span><br><span class="line">$ cd /opt</span><br><span class="line">$ sudo chmod -R 775 hbase #修改权限</span><br><span class="line">$ sudo chown -R hadoop:hadoop hbase #修改用户</span><br></pre></td></tr></table></figure>

<h3 id="3-修改hbase的环境变量"><a href="#3-修改hbase的环境变量" class="headerlink" title="3.修改hbase的环境变量"></a>3.修改hbase的环境变量</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim /opt/hbase/conf/hbase-env.sh</span><br><span class="line">$ export JAVA_HOME=/usr/lib/java</span><br></pre></td></tr></table></figure>

<p>在 /etc/profile下添加hbase的环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ export HBASE_HOME=/opt/hbase</span><br><span class="line">$ export PATH=$HBASE_HOME/bin:$PATH</span><br><span class="line">$ source /etc/profile</span><br></pre></td></tr></table></figure>

<h3 id="4-修改hbase-site-xml"><a href="#4-修改hbase-site-xml" class="headerlink" title="4.修改hbase-site.xml"></a>4.修改hbase-site.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ cd /opt/hbase/conf/</span><br><span class="line">$ sudo vim hbase-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">      &lt;property&gt;</span><br><span class="line">            &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hdfs://Master:9000/hbase&lt;/value&gt;</span><br><span class="line">      &lt;/property&gt;</span><br><span class="line">      &lt;property&gt;</span><br><span class="line">            &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">      &lt;/property&gt;</span><br><span class="line">      &lt;property&gt;</span><br><span class="line">            &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;Master,Slave1,Slave2&lt;/value&gt;</span><br><span class="line">      &lt;/property&gt;</span><br><span class="line">      &lt;property&gt;</span><br><span class="line">            &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;/home/hadoop&lt;/value&gt;</span><br><span class="line">      &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>其中第一个属性指定本机的hbase的存储目录，必须与Hadoop集群的core-site.xml文件配置保持一致；第二个属性指定hbase的运行模式，true代表全分布模式；第三个属性指定Zookeeper 管理的机器，一般为奇数个；第四个属性是数据存放的路径。这里我使用的默认的 HBase 自带的 Zookeeper。</p>
<h3 id="5-修改-opt-hbase-conf-下的regionservers"><a href="#5-修改-opt-hbase-conf-下的regionservers" class="headerlink" title="5.修改/opt/hbase/conf/下的regionservers"></a>5.修改/opt/hbase/conf/下的regionservers</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Slave1</span><br><span class="line">Slave2</span><br></pre></td></tr></table></figure>

<p>regionservers文件列出了所有运行hbase的机器（即HRegionServer)。此文件的配置和Hadoop中的slaves文件十分相似，每行指定一台机器的主机名。当HBase启动的时候，会将此文件中列出的所有机器启动。关闭时亦如此。</p>
<h3 id="6-修改ulimit限制"><a href="#6-修改ulimit限制" class="headerlink" title="6.修改ulimit限制"></a>6.修改ulimit限制</h3><p>HBase 会在同一时间打开大量的文件句柄和进程，超过 Linux 的默认限制，导致可能会出现错误。 所以编辑/etc/security/limits.conf文件，添加以下两行，提高能打开的句柄数量和进程数量。注意将hadoop改成你运行 HBase 的用户名。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop  -       nofile  65535</span><br><span class="line">hadoop  -       nproc   32000</span><br></pre></td></tr></table></figure>

<p>还需要在 /etc/pam.d/common-session 加上这一行:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session required pam_limits.so</span><br></pre></td></tr></table></figure>

<p>否则在/etc/security/limits.conf上的配置不会生效。 最后还要注销（logout或者exit）后再登录，这些配置才能生效！使用ulimit -n -u命令查看最大文件和进程数量是否改变了。记得在每台安装 HBase 的机器上运行哦。</p>
<h3 id="7-slave配置"><a href="#7-slave配置" class="headerlink" title="7.slave配置"></a>7.slave配置</h3><p>重复以上配置，最简单是直接复制过去即可</p>
<h3 id="8-启动hbase"><a href="#8-启动hbase" class="headerlink" title="8.启动hbase"></a>8.启动hbase</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ start-dfs.sh</span><br><span class="line">$ start-yarn.sh</span><br><span class="line">$ mr-jobhistory-daemon.sh start historyserver</span><br><span class="line">$ start-hbase.sh</span><br></pre></td></tr></table></figure>

<h3 id="9-验证是否安装成功"><a href="#9-验证是否安装成功" class="headerlink" title="9.验证是否安装成功"></a>9.验证是否安装成功</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">进入http://Master:16010验证是否安装成功</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://antfling.github.io/2016/12/31/HadoopInstall/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ant">
      <meta itemprop="description" content="Es Ist Vorbei">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Antfling">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/12/31/HadoopInstall/" class="post-title-link" itemprop="url">Hadoop安装教程</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2016-12-31 18:35:00" itemprop="dateCreated datePublished" datetime="2016-12-31T18:35:00+08:00">2016-12-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-09-29 14:52:07" itemprop="dateModified" datetime="2020-09-29T14:52:07+08:00">2020-09-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/BigData/" itemprop="url" rel="index"><span itemprop="name">BigData</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Hadoop安装教程"><a href="#Hadoop安装教程" class="headerlink" title="Hadoop安装教程"></a>Hadoop安装教程</h2><h3 id="1-在所有需要部署hadoop的机器上添加相同的用户作为hadoop的操作用户"><a href="#1-在所有需要部署hadoop的机器上添加相同的用户作为hadoop的操作用户" class="headerlink" title="1.在所有需要部署hadoop的机器上添加相同的用户作为hadoop的操作用户"></a>1.在所有需要部署hadoop的机器上添加相同的用户作为hadoop的操作用户</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a sudo adduser hadoop</span><br><span class="line">b password hadoop </span><br><span class="line">添加所有权限</span><br><span class="line">vim /etc/sudoers</span><br><span class="line">添加 hadoop ALL=(ALL:ALL) ALL 然后 :wq!退出</span><br></pre></td></tr></table></figure>

<h3 id="2-所有集群都需要免密码登录"><a href="#2-所有集群都需要免密码登录" class="headerlink" title="2.所有集群都需要免密码登录"></a>2.所有集群都需要免密码登录</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">su - hadoop</span><br><span class="line"></span><br><span class="line">$ ssh-keygen -t rsa</span><br><span class="line">$ cd .ssh/</span><br><span class="line">保证自己可以面密码</span><br><span class="line">$ cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line">上传到其他集群机器</span><br><span class="line">$ scp ~/.ssh/id_rsa.pub hadoop@hadoopslave1:/home/hadoop/</span><br><span class="line">$ scp ~/.ssh/id_rsa.pub hadoop@hadoopslave2:/home/hadoop/</span><br><span class="line">在其他节点做的工作</span><br><span class="line">$ mkdir ~/.ssh       # 如果不存在该文件夹需先创建，若已存在则忽略</span><br><span class="line">$ cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line">$ rm ~/id_rsa.pub    # 用完就可以删掉了</span><br></pre></td></tr></table></figure>

<h3 id="3-添加环境变量"><a href="#3-添加环境变量" class="headerlink" title="3.添加环境变量"></a>3.添加环境变量</h3><p>添加如下环境变量到 /etc/profile 然后source /etc/profile立即生效</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/lib/java</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib</span><br><span class="line">export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">export JAVA_HOME=/usr/lib/java</span><br><span class="line">export HADOOP_INSTALL=/usr/local/hadoop</span><br><span class="line">export PATH=$PATH:$HADOOP_INSTALL/bin</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_INSTALL/sbin</span><br><span class="line">export HADOOP_MAPRED_HOME=$HADOOP_INSTALL</span><br><span class="line">export HADOOP_COMMON_HOME=$HADOOP_INSTALL</span><br><span class="line">export HADOOP_HDFS_HOME=$HADOOP_INSTALL</span><br><span class="line">export YARN_HOME=$HADOOP_INSTALL</span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native</span><br><span class="line">export HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_INSTALL/lib&quot;</span><br></pre></td></tr></table></figure>

<h3 id="4-修改网络相关"><a href="#4-修改网络相关" class="headerlink" title="4 修改网络相关"></a>4 修改网络相关</h3><p>修改 vim /etc/network/interfaces 添加如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">address 192.168.1.98     #本机ip</span><br><span class="line">netmask 255.255.255.0 #网关</span><br><span class="line">gateway 192.168.1.1 #这个在/etc/hosts中配置的</span><br></pre></td></tr></table></figure>

<p>修改 vim /etc/resolv.conf 添加如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nameserver 192.168.1.1</span><br></pre></td></tr></table></figure>

<h3 id="5-将以上配置配置到其他节点"><a href="#5-将以上配置配置到其他节点" class="headerlink" title="5 将以上配置配置到其他节点"></a>5 将以上配置配置到其他节点</h3><h3 id="6-配置hadoop"><a href="#6-配置hadoop" class="headerlink" title="6 配置hadoop"></a>6 配置hadoop</h3><h4 id="a-配置主节点的hadoop"><a href="#a-配置主节点的hadoop" class="headerlink" title="a 配置主节点的hadoop"></a>a 配置主节点的hadoop</h4><h6 id="1-解决hadoop可能找不到JAVA-HOME的问题"><a href="#1-解决hadoop可能找不到JAVA-HOME的问题" class="headerlink" title="(1) 解决hadoop可能找不到JAVA_HOME的问题"></a>(1) 解决hadoop可能找不到JAVA_HOME的问题</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">解决方式是在hadoop/etc/hadoop/文件夹下的hadoop-env.sh中将JAVA_HOME写死,</span><br><span class="line">本例中为export JAVA_HOME=/usr/lib/java</span><br></pre></td></tr></table></figure>

<h6 id="2-配置slave-从节点，即datanode"><a href="#2-配置slave-从节点，即datanode" class="headerlink" title="(2) 配置slave(从节点，即datanode)"></a>(2) 配置slave(从节点，即datanode)</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">进入hadoop的配置文件下</span><br><span class="line">cd /user/local/hadoop/etc/hadoop </span><br><span class="line"></span><br><span class="line">slaves配置如下,只配置了两个从节点，主节点只作为NameNode使用</span><br><span class="line">ubuntu-2</span><br><span class="line">ubuntu-3</span><br></pre></td></tr></table></figure>

<h6 id="3-配置core-site-xml"><a href="#3-配置core-site-xml" class="headerlink" title="(3)配置core-site.xml"></a>(3)配置core-site.xml</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">内容如下</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://ubuntu-1:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;</span><br><span class="line">       &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<h6 id="4-配置hdfs-site-xml"><a href="#4-配置hdfs-site-xml" class="headerlink" title="(4) 配置hdfs-site.xml"></a>(4) 配置hdfs-site.xml</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">这里主要配置四个属性，两个namenode的和两个datanode的</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;ubuntu-1:50090&lt;/value&gt;&lt;!--nameNode的secondary配置--&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;2&lt;/value&gt;&lt;!--datenode节点数量的配置，默认是3，这里只配置两个--&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt;&lt;!--nameNode的name存放路径--&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;&lt;!--datenode的data存放路径--&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<h6 id="5-配置MapReduce-mapred-sit-xml"><a href="#5-配置MapReduce-mapred-sit-xml" class="headerlink" title="(5) 配置MapReduce:mapred-sit.xml"></a>(5) 配置MapReduce:mapred-sit.xml</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a 将mapred-sit.xml.template 复制出一份命名为mapred-sit.xml</span><br><span class="line">    cp mapred-sit.xml.template mapred-sit.xml</span><br><span class="line">b 然后编辑 mapred-sit.xml 添加如下配置</span><br><span class="line">    &lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;ubuntu-1:10020&lt;/value&gt;&lt;!--job历史运行记录端口-&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">            &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;ubuntu-1:19888&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">    &lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<h6 id="6-配置yarn-site-xml"><a href="#6-配置yarn-site-xml" class="headerlink" title="(6) 配置yarn-site.xml"></a>(6) 配置yarn-site.xml</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;ubuntu-1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<h6 id="7-将以上配置好的hadoop打包发送的其他节点上解压到相应的位置"><a href="#7-将以上配置好的hadoop打包发送的其他节点上解压到相应的位置" class="headerlink" title="(7) 将以上配置好的hadoop打包发送的其他节点上解压到相应的位置"></a>(7) 将以上配置好的hadoop打包发送的其他节点上解压到相应的位置</h6><h6 id="a-打包"><a href="#a-打包" class="headerlink" title="#a 打包"></a>#a 打包</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$sudo tar zcvf hadoop.tar.gz /usr/local/hadoop</span><br></pre></td></tr></table></figure>

<h6 id="b-传到各个节点"><a href="#b-传到各个节点" class="headerlink" title="#b 传到各个节点"></a>#b 传到各个节点</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ scp hadoop.tar.gz hadoop@ubunut-2:/home/hadoop</span><br><span class="line">$ scp hadoop.tar.gz hadoop@ubunut-3:/home/hadoop</span><br></pre></td></tr></table></figure>

<h6 id="c-各个节点解压放到-usr-local-hadoop"><a href="#c-各个节点解压放到-usr-local-hadoop" class="headerlink" title="#c 各个节点解压放到 /usr/local/hadoop"></a>#c 各个节点解压放到 /usr/local/hadoop</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo tar zxvf /home/hadoop/hadoop.tar.gz -C /usr/local</span><br></pre></td></tr></table></figure>

<h6 id="d-改变所属者"><a href="#d-改变所属者" class="headerlink" title="#d 改变所属者"></a>#d 改变所属者</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo chmod -R hadoop:hadoop /usr/local/hadoop</span><br></pre></td></tr></table></figure>

<h6 id="8-逐步启动hadoop"><a href="#8-逐步启动hadoop" class="headerlink" title="(8) 逐步启动hadoop"></a>(8) 逐步启动hadoop</h6><h6 id="a-先format-master节点上的hdfs-第一运行需要"><a href="#a-先format-master节点上的hdfs-第一运行需要" class="headerlink" title="#a 先format master节点上的hdfs (第一运行需要)"></a>#a 先format master节点上的hdfs (第一运行需要)</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs namenode format</span><br></pre></td></tr></table></figure>

<h6 id="b-开始启动hadoop-以下命令均在-usr-local-hadoop-sbin下面"><a href="#b-开始启动hadoop-以下命令均在-usr-local-hadoop-sbin下面" class="headerlink" title="#b 开始启动hadoop(以下命令均在/usr/local/hadoop/sbin下面)"></a>#b 开始启动hadoop(以下命令均在/usr/local/hadoop/sbin下面)</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ start-dfs.sh</span><br></pre></td></tr></table></figure>

<h6 id="c-在这里启动的时候可能出现一个警告如下"><a href="#c-在这里启动的时候可能出现一个警告如下" class="headerlink" title="#c 在这里启动的时候可能出现一个警告如下"></a>#c 在这里启动的时候可能出现一个警告如下</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br></pre></td></tr></table></figure>

<p>其实说的是现在使用的hadoop编译的时候用的c库和现在的linux环境上的不一致，解决方式是去如下链接 <a href="http://dl.bintray.com/sequenceiq/sequenceiq-bin" target="_blank" rel="noopener">http://dl.bintray.com/sequenceiq/sequenceiq-bin</a> 下载 hadoop-native-(version).tar， 然后执行如下命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo tar -x hadoop-native-(version).tar -C  hadoop/lib/native/</span><br></pre></td></tr></table></figure>

<p>然后在/etc/profile添加如下环境变量，立即生效即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ export JAVA_LIBRARY_PATH=/usr/local/hadoop/lib/native</span><br></pre></td></tr></table></figure>

<p>即可</p>
<h6 id="d-启动yarn"><a href="#d-启动yarn" class="headerlink" title="#d 启动yarn"></a>#d 启动yarn</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ start-yarn.sh</span><br></pre></td></tr></table></figure>

<h6 id="e-启动历史记录"><a href="#e-启动历史记录" class="headerlink" title="#e 启动历史记录"></a>#e 启动历史记录</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure>

<h6 id="f-如果是cenos发行版需要先关闭防火墙"><a href="#f-如果是cenos发行版需要先关闭防火墙" class="headerlink" title="#f 如果是cenos发行版需要先关闭防火墙"></a>#f 如果是cenos发行版需要先关闭防火墙</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#Centos6.X需要关闭防火墙</span><br><span class="line">$ sudo service iptables stop   # 关闭防火墙服务</span><br><span class="line">$ sudo chkconfig iptables off  # 禁止防火墙开机自启，就不用手动关闭了</span><br><span class="line">#Centos7需要关闭防火墙</span><br><span class="line">$ systemctl stop firewalld.service    # 关闭firewall</span><br><span class="line">$ systemctl disable firewalld.service # 禁止firewall开机启动</span><br></pre></td></tr></table></figure>

<h6 id="g-查看结果-分别在master和slave上运行jps查看不同结果"><a href="#g-查看结果-分别在master和slave上运行jps查看不同结果" class="headerlink" title="#g 查看结果 ,分别在master和slave上运行jps查看不同结果"></a>#g 查看结果 ,分别在master和slave上运行jps查看不同结果</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ jps </span><br><span class="line"># (slave服务器)结果如下 </span><br><span class="line">7392 NodeManager</span><br><span class="line">7569 Jps</span><br><span class="line">7235 DataNode</span><br><span class="line">#(master服务器)结果如下</span><br><span class="line">11714 Jps</span><br><span class="line">10988 NameNode</span><br><span class="line">11677 JobHistoryServer</span><br><span class="line">11389 ResourceManager</span><br><span class="line">11231 SecondaryNameNode</span><br></pre></td></tr></table></figure>

<h6 id="h-在master上运行如下命令看到结果"><a href="#h-在master上运行如下命令看到结果" class="headerlink" title="#h 在master上运行如下命令看到结果"></a>#h 在master上运行如下命令看到结果</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs dfsadmin -report</span><br></pre></td></tr></table></figure>

<p>结果如下</p>
<h6 id="i-在http-192-168-1-98-50070-上查看结果"><a href="#i-在http-192-168-1-98-50070-上查看结果" class="headerlink" title="#i 在http://192.168.1.98:50070/上查看结果"></a>#i 在<a href="http://192.168.1.98:50070/%E4%B8%8A%E6%9F%A5%E7%9C%8B%E7%BB%93%E6%9E%9C" target="_blank" rel="noopener">http://192.168.1.98:50070/上查看结果</a></h6><h3 id="7-时间同步"><a href="#7-时间同步" class="headerlink" title="7 时间同步"></a>7 时间同步</h3><p>一般是安装在slave节点</p>
<h5 id="a-在线安装"><a href="#a-在线安装" class="headerlink" title="a 在线安装"></a>a 在线安装</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install ntp</span><br></pre></td></tr></table></figure>

<h5 id="b-离线安装"><a href="#b-离线安装" class="headerlink" title="b 离线安装"></a>b 离线安装</h5><p>如果要离线安装，那么就需要下载ntp安装包和依赖包。我们可以在一个有线环境下运行上面的在线安装，然后到/var/cache/apt/archives这个目录下拷贝完整的ntp安装包和依赖包。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ dpkg  -i  libopts25_1%3a5.12-0.1ubuntu1_amd64.deb </span><br><span class="line">$ dpkg  -i  ntp_1%3a4.2.6.p3+dfsg-1ubuntu3.1_amd64.deb</span><br></pre></td></tr></table></figure>

<p>当然还有更加简单的方法，将下载的deb包拷贝到/var/cache/apt/archives目录下，然后在执行一下命令同样可以安装。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install ntp</span><br></pre></td></tr></table></figure>

<h5 id="c-服务检测"><a href="#c-服务检测" class="headerlink" title="c 服务检测"></a>c 服务检测</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo service --status-all</span><br></pre></td></tr></table></figure>

<p>结果如下，ntp前面为+号表示已经启动服务 <img src="http://i.imgur.com/9sfLA8z.png" alt="img"></p>
<h5 id="d-配置修改-etc-ntp-conf"><a href="#d-配置修改-etc-ntp-conf" class="headerlink" title="d 配置修改(/etc/ntp.conf)"></a>d 配置修改(/etc/ntp.conf)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">主要是修改此文件下的，修改之前建议备份</span><br><span class="line"># Clients from this (example!) subnet have unlimited access, but only if</span><br><span class="line"># cryptographically authenticated.</span><br><span class="line">restrict 192.168.1.0 mask 255.255.255.0 nomodify</span><br></pre></td></tr></table></figure>

<h5 id="f-时间同步"><a href="#f-时间同步" class="headerlink" title="f 时间同步"></a>f 时间同步</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ntpdate ubuntu-1</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://antfling.github.io/2016/12/31/RabbitMQ/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ant">
      <meta itemprop="description" content="Es Ist Vorbei">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Antfling">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/12/31/RabbitMQ/" class="post-title-link" itemprop="url">RabbitMQ</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2016-12-31 18:35:00" itemprop="dateCreated datePublished" datetime="2016-12-31T18:35:00+08:00">2016-12-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-09-29 14:52:07" itemprop="dateModified" datetime="2020-09-29T14:52:07+08:00">2020-09-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/message/" itemprop="url" rel="index"><span itemprop="name">message</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="RabbitMQ"><a href="#RabbitMQ" class="headerlink" title="RabbitMQ"></a>RabbitMQ</h2><h3 id="RabbitMQ是什么，从kafka说起"><a href="#RabbitMQ是什么，从kafka说起" class="headerlink" title="RabbitMQ是什么，从kafka说起"></a>RabbitMQ是什么，从kafka说起</h3><p>kafka是一个消息式的日志服务系统，为什么说是消息式的日志服务系统，因为它本身就是是消息系统，但是由于它的消息处理机制以及在hadoop中的作用，它是作为一个日志服务系统而存在的。(可以有九成的把握说,hadoop除却hdfs的文件管理，剩下的就是写日志和处理日志)<br>在这里首先介绍下kafka的消息处理机制，在介绍之前先简单梳理下kafka的结构。<br>kafka本身作为消息系统，肯定是具备了消息系统的一般架构，而它的消息处理流程简单概括如下图所示<br><img src="http://kafka.apache.org/images/producer_consumer.png" alt><br>从上图可以看到，kafka接收制造者发过来的消息，然后简单处理，转发给消费者。<br>然后我们来看看kafka的具体结构:</p>
<h3 id="kafka的结构"><a href="#kafka的结构" class="headerlink" title="kafka的结构"></a>kafka的结构</h3><p>在了解结构之前先了解kafka中的几个简单概念</p>
<ul>
<li><strong>Topic</strong> kafka按照消息的种类保存叫做topic(不同种类的消息保存在不同的topic下)* <strong>Producers</strong> 将消息发送到一个kafaka的topic上的过程叫做生产过程(producers)* <strong>Consumers</strong> 订阅topic，并处理topic中存储的信息的过程叫做消费过程(consumers)* <strong>Brocker</strong> 一个集群上的一个或者多个kafka服务器，每一个都被成为代理(brocker)<br>除了上面四个外，在又在topic中添加了分区的概念，而分区的出现时为了解决消息存储写入水平扩展的问题。每个broker有几个partition，这样即使是一个topic下的消息也会被写入到不同的partition下，可以被均匀的分配到整个集群当中</li>
<li><strong>partition</strong><br>在partition中加入了segment,每个segment都有限定大小，一旦超过限制的大小，便不会把消息写入到这个segment中，二是写入到此分区下的新的segment中.当需要做数据清除的时候需要删除旧的segment即可</li>
<li><strong>segment</strong></li>
</ul>
<h3 id="读写机制"><a href="#读写机制" class="headerlink" title="读写机制"></a>读写机制</h3><h4 id="写"><a href="#写" class="headerlink" title="写"></a>写</h4><p>然后看下面这张图来详细介绍kafka的消息机制。</p>
<p><img src="http://kafka.apache.org/images/log_anatomy.png" alt></p>
<p>从上图可以看出，过来的消息，kafka都是按照时间的顺序写入到每个topic下的不同partition下.用我上小学老师对我写的日记评价，其实就是在记流水账。记流水账的好处就是没有复杂的存储逻辑，来一个消息只需要保存就可以了。(这里可以介绍下segment，并介绍下kafka的存储所有消息)</p>
<h4 id="读"><a href="#读" class="headerlink" title="读"></a>读</h4><p>同样是上面的图，kafka的读取消息是从左往右读的.每当消费者要来读取消息的时候都会带一个起始位置信息(时间点)，然后从起始位置往右读.这样的话kafka也不需要怎么处理读取逻辑，只需要告诉kafka从哪里开始读即可.<br>但是并不是所有的消息都是一直有效的，kafka默认有个时间过期限制，默认是两天，一旦存储时间超过两天，这个消息便算过期了，即使是消费者要来读取消息也是不能的。这样消费者就只能去处理未过期的数据。<br>(这里可能有个疑问就是，如果消费者处理的日志速度确实很慢，那么出现这种情况要怎么解决。这里其实是交给yarn去判断的，yarn会定期检查任务的处理进度，一旦有个节点(brocker)速度的处理速度过慢，那么这个节点的的任务就会交给处理快的节点去处理，这个节点的处理结果是无效的。并且提醒运维人员去检查节点.)<br>后续的就是消费者(即MapReduce),将日志内容解析成任务然后进行处理。</p>
<h4 id="kafka的无状态"><a href="#kafka的无状态" class="headerlink" title="kafka的无状态"></a>kafka的无状态</h4><p>从上面可以看出，kafka本身往小的说是只负责读写日志的。而且kafka根本不维护自己的brocker(节点)和消费者的集群，这些全部交给zookeeper进行维护。正是因为kafka的读写简单，和本身不保存处理进度的状态，这样造成的结果就是，kafka能够大批量的处理读写操作，有着很大的吞吐量。<br>可以看到下面的拓扑结构图<br><img src="http://i.imgur.com/IC47WKD.png" alt></p>
<h3 id="通过kafka对比rabbitmq"><a href="#通过kafka对比rabbitmq" class="headerlink" title="通过kafka对比rabbitmq"></a>通过kafka对比rabbitmq</h3><p>上面简单介绍了kafka的结构和消息处理机制，然后通过对比来介绍下rabbitmq对此做了那些改变和这些改变造成的结果。<br>通过下图，简单了解校rabbitmq的结构<br><img src="http://i.imgur.com/8p1354s.png" alt><br>在rabbitmq中，本身集群的维护，和消费者的连接，和生产者的连接都是rabbitmq来进行维护的.这样做的目的其实确保rabbitmq有着消息的绝对管理权.<br>其次，kafka中的topic，在rabbitmq中可以看作是exchange，并且将partition和segment这两个进行抽象和分离，一部分成了queue，另外一部分合并到exchange中的route中.<br>关于producers和consumers这两个没有多大变化.</p>
<h3 id="rabbitmq有哪些变化"><a href="#rabbitmq有哪些变化" class="headerlink" title="rabbitmq有哪些变化"></a>rabbitmq有哪些变化</h3><p>下面来说说在rabbitmq中的消息处理有那些变化.<br>在kafka中消息是写入到topic下的partition的segment文件中的。在rabbitmq中，消息是发送到exchange下然后根据route分配到queue中.可以看到至少在表面上是没有多大区别的.<br>上面已经介绍了kafka是怎么样写入到segment中。下面就详细介绍rabbitmq是怎么把消息写入到queue中.<br>在介绍的时候根据rabbitmq的四个不同的消息转发器exchange来分别介绍，Fanout(扇形),Direct（直流）,Topic(主题),Headers（头）。</p>
<ul>
<li><strong>Fanout(扇形消息转发器)</strong><br><img src="http://i.imgur.com/7ndy5Ts.png" alt><br>这个一看就是很简单的，消息放到exchange中(具体怎么放，这个rabbitmq有专门的api，直接调用push进去就可以了),然后这里不管你有没有设置route，只要是这个exchange上面有的queue，每个队列都会获得一份，这个是无差别的.</li>
<li><strong>Direct(直流消息转发器)</strong><br><img src="http://i.imgur.com/rlv3mVq.png" alt><br>这个比Fanout的稍微严格了一点，每个queue都设置了一个接收的route，direct将带有不同route的的message分发到匹配的queue上面，没有route的message在这里将会被舍弃。</li>
<li><strong>Topic(主题消息转发器)</strong><br><img src="http://i.imgur.com/aYgTHVA.png" alt><br>这边的其实是在Direct的基础上又改了下，因为有时候只是简单的定向发送到队列是不行的，需要根据消息的种类将消息分类，然后再继续分发消息.<br>这边需要讲一下的就是这里的route的匹配规则</li>
<li>.与.之间的看作一个单词* *是只匹配一个单词* # 是匹配一个到多个单词</li>
<li><strong>Headers(头消息转发器)</strong><br>这里无图(黑板画图)<br>如果知道http的请求，可以知道请求一个接口的时候是可以发在请求中带一个请求头(Headers)的。<br>一个message可以带着很多请求头,每个请求头都是以(key-value)键值对的形式存在,每个queue可以接受多个header.<br>请求头中的信息是可以随意定的，毕竟虽然route已经可以将很多消息进行过滤分发，但是还是有些情况是route做不到的，例如版本升级，还要兼容老的版本，你不可能再做另外一个版本出来，只是需要在要改的地方添加不同版本的即可。这样可以把版本信息放到Header中，然后进行消息分发.</li>
</ul>
<h3 id="rabbitmq的理想模式"><a href="#rabbitmq的理想模式" class="headerlink" title="rabbitmq的理想模式"></a>rabbitmq的理想模式</h3><p>通过以上介绍，可以介绍下rabbitmq的一个完整的消息发送模式.</p>
<figure class="highlight plain"><figcaption><span>-> Headers Exchange-> Topic Exchange -> Direct Exchange -> Fanout Exchange -> Queue -> C```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### Queue的消息处理机制</span><br><span class="line"></span><br><span class="line">在rabbitmq中 queue中的消息是分为持久化存储和非持久化存储，持久化存储是和kafka类似，直接写在硬盘上面，非持久化存储是直接保存在内存中的，内存满了才会临时写入到硬盘上面.</span><br><span class="line">虽然rabbitmq和kafka类似都是以堆栈的形式保存消息，但是rabbitmq中Queue保存的message是不带有时间戳信息，只是按照先后顺序保存在queue中.</span><br><span class="line">在rabbitmq中，消费者监听queue中的动态，一旦有消息存在的时候便开始处理。在处理的过程中这个消息后面的消息是不被消费的，直到这个消息被消费者返回了已经确实消费完的状态才会继续消费下一条消息.</span><br><span class="line"></span><br><span class="line">### 队列之间的通信</span><br><span class="line"></span><br><span class="line">由于rabbitmq有消息消费确认机制，所以在rabbitmq中是可以进行队列之间通信的.所谓的队列之间的通信其实是充分利用了消息确认机制.</span><br><span class="line">举个邮件发送的例子.有个sendmail队列是专门用来发送邮件，有个reply队列主要处理消息处理完毕返回消息的队列。同样的sendmail也有消费者，reply消费者默认是不指定的.我这边发送一个简历发送的请求到sendmail中，sendmail的消费者处理完毕后返回一个对象，这个对象包含邮件是否发送成功等结果.而这个对象默认是到reply队列中,我这边等待的其实是reply队列中关于sendmail消息处理的结果。然后我这边得到结果</span><br><span class="line"></span><br><span class="line">### kafka和RabbitMQ总结</span><br><span class="line">* **Kafka**</span><br><span class="line">  ```1.消息日志读写简单2.消息处理大批量，高并发3.可用于集群大批量对结果要求不那么严谨应用之间的信息交换</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>RabbitMQ</strong><br><code>1.加强了消息来源的可靠性,（主要是Exchange的消息的分发和过滤）2.加强了消息处理结果的可靠性,(增加了消息确认机制)3.用于应用系统之间的通信，用于系统服务的解耦,和处理一些较为耗时的操作（例如，投递一份简历，用户只需要知道简历已经在投递队列就可以了，不需要立即知道投递结果）</code></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://antfling.github.io/2016/12/31/FaceBookATCInstallAndUse/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ant">
      <meta itemprop="description" content="Es Ist Vorbei">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Antfling">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/12/31/FaceBookATCInstallAndUse/" class="post-title-link" itemprop="url">FaceBook Augmented-Traffic-Control 安装及使用文档</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2016-12-31 18:35:00" itemprop="dateCreated datePublished" datetime="2016-12-31T18:35:00+08:00">2016-12-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-09-29 14:52:07" itemprop="dateModified" datetime="2020-09-29T14:52:07+08:00">2020-09-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="FaceBook-Augmented-Traffic-Control-安装及使用文档"><a href="#FaceBook-Augmented-Traffic-Control-安装及使用文档" class="headerlink" title="FaceBook Augmented-Traffic-Control 安装及使用文档"></a>FaceBook Augmented-Traffic-Control 安装及使用文档</h2><h3 id="运行环境"><a href="#运行环境" class="headerlink" title="运行环境"></a>运行环境</h3><p>linux环境,可使用虚拟机安装一个linux环境出来，或者在一台linux服务器上安装</p>
<h3 id="安装前准备"><a href="#安装前准备" class="headerlink" title="安装前准备"></a>安装前准备</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install python-pip python-dev build-essential）</span><br><span class="line">#pytho环境安装及更新 </span><br><span class="line">$ sudo pip install --upgrade pip</span><br></pre></td></tr></table></figure>

<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#使用pip直接安装ATC所有组组件</span><br><span class="line">$ pip install atc_thrift atcd django-atc-api django-atc-demo-ui django-atc-profile-storage</span><br></pre></td></tr></table></figure>

<h3 id="安装后运行相关部署"><a href="#安装后运行相关部署" class="headerlink" title="安装后运行相关部署"></a>安装后运行相关部署</h3><p>1.使用django-admin生成一个web工程</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ django-admin startproject atcui cd atcui</span><br></pre></td></tr></table></figure>

<p>2.atcui的相关设置</p>
<ul>
<li><p>修改atcui/settings.py 加入如下内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSTALLED_APPS = (...# Django ATC API&apos;rest_framework&apos;,&apos;atc_api&apos;,# Django ATC Demo UI&apos;bootstrap_themes&apos;,&apos;django_static_jquery&apos;,&apos;atc_demo_ui&apos;,# Django ATC Profile Storage&apos;atc_profile_storage&apos;,)</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改atcui/urls.py，urlpatterns 中加入atc的url页面</p>
<figure class="highlight plain"><figcaption><span>django.views.generic.base import RedirectView</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urlpatterns = patterns(&apos;&apos;,...# Django ATC APIurl(r&apos;^api/v1/&apos;, include(&apos;atc_api.urls&apos;)),# Django ATC Demo UIurl(r&apos;^atc_demo_ui/&apos;, include(&apos;atc_demo_ui.urls&apos;)),# Django ATC profile storageurl(r&apos;^api/v1/profiles/&apos;, include(&apos;atc_profile_storage.urls&apos;)),url(r&apos;^$&apos;, RedirectView.as_view(url=&apos;/atc_demo_ui/&apos;, permanent=False)),)</span><br></pre></td></tr></table></figure>
</li>
<li><p>更新下数据库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python manage.py migrate</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo atcd --atcd-lan wlan0 # wlan0 是无限网卡的名字 sudo python manage.py runserver 0.0.0.0:8000</span><br></pre></td></tr></table></figure>

<h3 id="查看"><a href="#查看" class="headerlink" title="查看"></a>查看</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">进入webui界面进行查看 htt://localhost:80000</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://antfling.github.io/2016/12/31/dockerDataColumn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ant">
      <meta itemprop="description" content="Es Ist Vorbei">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Antfling">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/12/31/dockerDataColumn/" class="post-title-link" itemprop="url">数据卷容器创建及跨服务器连接</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2016-12-31 18:26:00" itemprop="dateCreated datePublished" datetime="2016-12-31T18:26:00+08:00">2016-12-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-09-29 14:52:07" itemprop="dateModified" datetime="2020-09-29T14:52:07+08:00">2020-09-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="docker-数据卷容器创建及跨服务器连接"><a href="#docker-数据卷容器创建及跨服务器连接" class="headerlink" title="docker 数据卷容器创建及跨服务器连接"></a>docker 数据卷容器创建及跨服务器连接</h2><h3 id="创建数据卷容器"><a href="#创建数据卷容器" class="headerlink" title="创建数据卷容器"></a>创建数据卷容器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 创建一个名为dbstore的数据卷容器，但是不启动，挂在/dbdata数据卷</span><br><span class="line">$ docker create -v /dbdata --name dbstore ubuntu:latest /bin/bash</span><br><span class="line">#启动自己的镜像连接dbstore数据卷容器</span><br><span class="line">$ docker run -d --volumes-from dbstore --name db1 my_image</span><br><span class="line">#可进去查看挂在卷</span><br></pre></td></tr></table></figure>

<h3 id="跨服务器连接数据卷"><a href="#跨服务器连接数据卷" class="headerlink" title="跨服务器连接数据卷"></a>跨服务器连接数据卷</h3><p>参考<a href="https://www.gitbook.com/book/airlove/big-data/edit#" target="_blank" rel="noopener">文档</a></p>
<h4 id="1-启动一个btsync服务器"><a href="#1-启动一个btsync服务器" class="headerlink" title="1.启动一个btsync服务器"></a>1.启动一个btsync服务器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d -p 8888:8888 -p 55555:55555 --name btsync ctlc/btsync</span><br></pre></td></tr></table></figure>

<h4 id="2-查看logs-找到secrct"><a href="#2-查看logs-找到secrct" class="headerlink" title="2.查看logs 找到secrct"></a>2.查看logs 找到secrct</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker logs btsync</span><br></pre></td></tr></table></figure>

<h4 id="3-显示如下"><a href="#3-显示如下" class="headerlink" title="3.显示如下"></a>3.显示如下</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Starting btsync with secret: A6ZUOIORDYDFAPKGUCRXJDKJWAQKAYVQA</span><br><span class="line">By using this application, you agree to our Privacy Policy and Terms.</span><br><span class="line">http://www.bittorrent.com/legal/privacy</span><br><span class="line">http://www.bittorrent.com/legal/terms-of-use</span><br><span class="line"></span><br><span class="line">total physical memory 536870912 max disk cache 2097152</span><br><span class="line">Using IP address 10.103.100.4</span><br></pre></td></tr></table></figure>

<h4 id="4-创建一个容器共享上面容器的数据卷"><a href="#4-创建一个容器共享上面容器的数据卷" class="headerlink" title="4.创建一个容器共享上面容器的数据卷"></a>4.创建一个容器共享上面容器的数据卷</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -i -t --volumes-from btsync ubuntu /bin/bash</span><br></pre></td></tr></table></figure>

<h4 id="5-在新建的容器-data-数据卷下创建一个文件"><a href="#5-在新建的容器-data-数据卷下创建一个文件" class="headerlink" title="5.在新建的容器/data 数据卷下创建一个文件"></a>5.在新建的容器/data 数据卷下创建一个文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ touch /data/shared_from_server_one</span><br></pre></td></tr></table></figure>

<h4 id="6-在最出建的容器btsync里面查看-data文件"><a href="#6-在最出建的容器btsync里面查看-data文件" class="headerlink" title="6.在最出建的容器btsync里面查看/data文件"></a>6.在最出建的容器btsync里面查看/data文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ root@btsync:ls /data</span><br></pre></td></tr></table></figure>

<h4 id="7-在另一个主机创建容器并使用secret"><a href="#7-在另一个主机创建容器并使用secret" class="headerlink" title="7.在另一个主机创建容器并使用secret"></a>7.在另一个主机创建容器并使用secret</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d --name btsync-client -p 8888:8888 -p 55555:55555 ctlc/btsync A6ZUOIORDYDFAPKGUCRXJDKJWAQKAYVQA</span><br></pre></td></tr></table></figure>

<h4 id="8-创建一个容器共享上面容器的数据卷"><a href="#8-创建一个容器共享上面容器的数据卷" class="headerlink" title="8.创建一个容器共享上面容器的数据卷"></a>8.创建一个容器共享上面容器的数据卷</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -i -t --volumes-from btsync-client ubuntu /bin/bash</span><br></pre></td></tr></table></figure>

<h4 id="9-在新建的容器-data-数据卷下创建一个文件"><a href="#9-在新建的容器-data-数据卷下创建一个文件" class="headerlink" title="9.在新建的容器/data 数据卷下创建一个文件"></a>9.在新建的容器/data 数据卷下创建一个文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ touch /data/shared_from_server_two</span><br><span class="line">#此时使用ls可以看到两个文件shared_from_server_two 和shared_from_server_one</span><br><span class="line">$ ls /data</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ant</p>
  <div class="site-description" itemprop="description">Es Ist Vorbei</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">35</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ant</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  















  

  

</body>
<script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.5.1.min.js" ></script> 
<script type="text/javascript"> 
/* 鼠标特效 */
var a_idx = 0; 
jQuery(document).ready(function($) { 
    $("body").click(function(e) { 
        var a = new Array("富强", "民主", "文明", "和谐", "自由", "平等", "公正" ,"法治", "爱国", "敬业", "诚信", "友善"); 
        var $i = $("<span/>").text(a[a_idx]); 
        a_idx = (a_idx + 1) % a.length; 
        var x = e.pageX, 
        y = e.pageY; 
        $i.css({ 
            "z-index": 999999999999999999999999999999999999999999999999999999999999999999999, 
            "top": y - 20, 
            "left": x, 
            "position": "absolute", 
            "font-weight": "bold", 
            "color": "#ff6651" 
        }); 
        $("body").append($i); 
        $i.animate({ 
            "top": y - 180, 
            "opacity": 0 
        }, 
        1500, 
        function() { 
            $i.remove(); 
        }); 
    }); 
}); 
</script>
</html>
